{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325d6aa2",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "This chapter:  train/eval/finetine/optimize w/ pytorch. Then, optuna library for fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eebf98",
   "metadata": {},
   "source": [
    "## Fundamentals\n",
    "\n",
    "Data type is a tensor. Its a multi-dim array w/shape and datatype. Can live on GPU, and does auto-differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe3840",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b58dd111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32245ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors only take one data type. If you give it more than one, the most general type will be selected (complex > float > int > bool)\n",
    "display(X.dtype)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6a94ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66., 56.],\n",
       "        [56., 49.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntax\n",
    "X[:,1]\n",
    "10 * (X+1)\n",
    "X.exp() #item-wise exponential\n",
    "X.mean(dim=0) # col-wise mean\n",
    "X @ X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "91f101ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.tensor(X.numpy(), dtype=torch.float32)\n",
    "# you can convert btwn numpy and torch easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c85c294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 99.,  7.],\n",
       "        [ 2., 99.,  6.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can modify in place\n",
    "X[:,1] = 99\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c3049dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 99.,  7.],\n",
       "        [ 0., 99.,  6.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0] = -5\n",
    "X.relu_() \n",
    "# Methods ending in _ are in place, normal methods are not in place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c720f7",
   "metadata": {},
   "source": [
    "### Hardware Acceleration\n",
    "\n",
    "PyTorch has accelerator support for intel, apple, nvidia, amd, etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4ac47a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a gpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"We have a gpu\")\n",
    "\n",
    "M = torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float32)\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b9292fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device\n",
    "# There are multiple ways to put tensors on gpus, like .cuda(), or setting device= param in torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5300cf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17660ac3",
   "metadata": {},
   "source": [
    "If your neural net is deep, GPU speed and RAM matters most, if its shallow, getting training data onto GPU is the bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d0e893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 ms ± 3.01 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000,1000))\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee3d1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651 µs ± 13.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000,1000), device=\"cuda\")\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8db1fc",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "PyTorch does reverse-mode auto-diff (ch9) quickly with a method called autograd (auto gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3411b4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True) # requires_grad tells pytorch to keep track of computations for backpropagation\n",
    "f = x**2 # keeps a grad_fn= argument to tell pytorch how to backpropagate through this\n",
    "f.backward() # computes gradients\n",
    "x.grad\n",
    "# the derivative of x**2 at x=5 is in fact 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3217e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do gradient descent, you need to tell pytorch not to track this step\n",
    "# Otherwise it would include it in backprop\n",
    "\n",
    "lr = .1\n",
    "with torch.no_grad():\n",
    "    x -= lr*x.grad\n",
    "\n",
    "# This code is equivalent: (x detached shares memory with x)\n",
    "# x_detached = x.detach()\n",
    "# x_detached -= lr*x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e3bcfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before you repeat the forward > backward > gradient descent step, need to set gradients to 0\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f1038ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8026e-45, requires_grad=True)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The whole training loop:\n",
    "lr = .1\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "for iter in range(500):\n",
    "    f = x**2\n",
    "    f.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= lr*x.grad\n",
    "    x.grad.zero_()\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7385a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use in-place operations to save memory you have to be careful\n",
    "# Autograd doesnt let you do an in-place op to a leaf node\n",
    "\n",
    "t = torch.tensor(2.0, requires_grad=True)\n",
    "Z = t.exp() # intermediate step\n",
    "Z+=1 # in place operation (pytorch has no idea where to keep the computation graph for both steps)\n",
    "# Z.backward() #-> throws error\n",
    "\n",
    "# you need to do Z = Z+1, it creates a new step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a8342",
   "metadata": {},
   "source": [
    "PyTorch stores different operations differently\n",
    "- exp(), relu(), sqrt(), sigmoid(), tanh() save output in computation graph during the forward pass. You cannot modify their output in place.\n",
    "- abs(), cos(), log() save their inputs, so you cant change whatever you input to them before the backward pass\n",
    "- max(), min(), sgn(), std() save inputs and outputs, so do not change their inputs or outputs in place before .backward()\n",
    "- ceil(), floor(), mean(), sum() store nothing. Do what you want\n",
    "\n",
    "Generally, make your models without in-place ops, then if you need to speed up or save memory you can convert to in-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004dce5f",
   "metadata": {},
   "source": [
    "## Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3de74844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = fetch_california_housing(as_frame=False)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(data.data, data.target, test_size=.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "02090d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "means = X_train.mean(dim=0, keepdims=True)\n",
    "stds = X_train.std(dim=0, keepdims=True)\n",
    "# stdizing\n",
    "X_train = (X_train-means)/stds\n",
    "X_valid = (X_valid-means)/stds\n",
    "X_test = (X_test-means)/stds\n",
    "\n",
    "y_train = torch.FloatTensor(y_train).reshape(-1,1)\n",
    "y_valid = torch.FloatTensor(y_valid).reshape(-1,1)\n",
    "y_test = torch.FloatTensor(y_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "55f63121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 16.12990951538086\n",
      "Epoch 1 loss: 4.828019618988037\n",
      "Epoch 2 loss: 2.190487861633301\n",
      "Epoch 3 loss: 1.2756378650665283\n",
      "Epoch 4 loss: 0.9258849024772644\n",
      "Epoch 5 loss: 0.7817054390907288\n",
      "Epoch 6 loss: 0.7151285409927368\n",
      "Epoch 7 loss: 0.679024875164032\n",
      "Epoch 8 loss: 0.6556843519210815\n",
      "Epoch 9 loss: 0.6383019685745239\n",
      "Epoch 10 loss: 0.6241695284843445\n",
      "Epoch 11 loss: 0.6121392250061035\n",
      "Epoch 12 loss: 0.6016687154769897\n",
      "Epoch 13 loss: 0.5924591422080994\n",
      "Epoch 14 loss: 0.5843158960342407\n",
      "Epoch 15 loss: 0.5770943760871887\n",
      "Epoch 16 loss: 0.5706777572631836\n",
      "Epoch 17 loss: 0.5649677515029907\n",
      "Epoch 18 loss: 0.5598797798156738\n",
      "Epoch 19 loss: 0.5553401112556458\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n = X_train.shape[1] # n features\n",
    "w = torch.randn((n,1), requires_grad=True) # weights\n",
    "b = torch.tensor(0., requires_grad=True) # biases\n",
    "\n",
    "lr = .4\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = X_train @ w + b\n",
    "    loss = ((y_pred - y_train) ** 2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        b -= b.grad * lr\n",
    "        w -= w.grad * lr\n",
    "        b.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    print(f\"Epoch {epoch} loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "288d10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7111],\n",
      "        [0.9956],\n",
      "        [1.2532]])\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "with torch.no_grad():\n",
    "    print(X_test[:3] @ w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179ee4c",
   "metadata": {},
   "source": [
    "This works but PyTorch has a higher level API to do all this easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d3881",
   "metadata": {},
   "source": [
    "#### PyTorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0c1f594b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Linear(in_features=n, out_features=1)\n",
    "model.weight #.weight and .bias are children of torch.nn.Parameter, which is a child of torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cc0a7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3117], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e2919933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6250]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:1])\n",
    "# not trained yet so predictions r random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2bb5f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pick an optimizier and a loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n",
    "    tenth = n_epochs //10\n",
    "    for epoch in range(n_epochs):\n",
    "        yPred = model(X_train)\n",
    "        loss = criterion(y_train, yPred)\n",
    "        loss.backward()\n",
    "        optimizer.step() # updates b, w\n",
    "        optimizer.zero_grad()\n",
    "        if(epoch % tenth == 0):\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "51a9d803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.2368388175964355\n",
      "Epoch 20, Loss: 0.5237898826599121\n",
      "Epoch 40, Loss: 0.5146560072898865\n",
      "Epoch 60, Loss: 0.513292670249939\n",
      "Epoch 80, Loss: 0.5130218267440796\n",
      "Epoch 100, Loss: 0.5129623413085938\n",
      "Epoch 120, Loss: 0.5129488706588745\n",
      "Epoch 140, Loss: 0.5129457712173462\n",
      "Epoch 160, Loss: 0.5129451155662537\n",
      "Epoch 180, Loss: 0.5129449367523193\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "train_bgd(model, optimizer, mse, X_train, y_train, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "069b90e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7168],\n",
       "        [0.9449],\n",
       "        [1.1635]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_new)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941fd08",
   "metadata": {},
   "source": [
    "## Implementing a Regression MLP\n",
    "\n",
    "pytorch has `nn.Sequential` that lets you chain modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6d431621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(n, 50), # n inputs and any number of outputs\n",
    "    nn.ReLU(), # shape of output = shape input. just an activation function\n",
    "    nn.Linear(50,40), # inputs of 2 must equal outputs of 1. number of outputs can be whatever you want though\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40,1) # final n outputs must match targets dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ac05af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.844911098480225\n",
      "Epoch 100, Loss: 0.6239899396896362\n",
      "Epoch 200, Loss: 0.44056236743927\n",
      "Epoch 300, Loss: 0.41409310698509216\n",
      "Epoch 400, Loss: 0.39982160925865173\n",
      "Epoch 500, Loss: 0.38740676641464233\n",
      "Epoch 600, Loss: 0.37716424465179443\n",
      "Epoch 700, Loss: 0.36810338497161865\n",
      "Epoch 800, Loss: 0.3594208359718323\n",
      "Epoch 900, Loss: 0.3518669605255127\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "\n",
    "train_bgd(model, optimizer, mse, X_train, y_train, 1000)\n",
    "print(next(model.parameters()).device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d19ae8",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent w/ DataLoaders\n",
    "\n",
    "Torch has a `torch.utils.data.DataLoader` class that efficiently loads data and shuffles if we want it to\n",
    "\n",
    "DataLoaders expects the dataset to have a len() and getitem() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "961423f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n, 50), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(50,40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40,1) \n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "63f156f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train() # this switched modules to training mode, doesnt matter rn but it will later\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_batch, y_pred)\n",
    "            total_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d6e3b0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.5512\n",
      "Epoch 2/20, Loss: 0.4332\n",
      "Epoch 3/20, Loss: 0.4005\n",
      "Epoch 4/20, Loss: 0.3850\n",
      "Epoch 5/20, Loss: 0.3608\n",
      "Epoch 6/20, Loss: 0.3594\n",
      "Epoch 7/20, Loss: 0.3470\n",
      "Epoch 8/20, Loss: 0.3390\n",
      "Epoch 9/20, Loss: 0.3317\n",
      "Epoch 10/20, Loss: 0.3261\n",
      "Epoch 11/20, Loss: 0.3226\n",
      "Epoch 12/20, Loss: 0.3943\n",
      "Epoch 13/20, Loss: 0.3429\n",
      "Epoch 14/20, Loss: 0.3295\n",
      "Epoch 15/20, Loss: 0.3286\n",
      "Epoch 16/20, Loss: 0.3121\n",
      "Epoch 17/20, Loss: 0.3163\n",
      "Epoch 18/20, Loss: 0.3074\n",
      "Epoch 19/20, Loss: 0.3028\n",
      "Epoch 20/20, Loss: 0.2991\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, mse, train_loader, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41425fb",
   "metadata": {},
   "source": [
    "- You can set `pin_memory=True` in the dataloader to speed up training and the cost of more cpu ram\n",
    "    - Also set `non_blocking=True` in the .to() method to avoid blocking cpu during data transfer\n",
    "\n",
    "- This training loop waits until one batch is done to load another. Set dataloaders `num_workers=` to add workers, and tweak number of batches fetched with `prefetch_factor`. Windows sometimes lags with this, so set `persistent_workers=True` to reuse workers \n",
    "\n",
    "- Note that it seems like these arguments have issues in juypter notebook and you might have to work in .py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25925978",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "02638849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, metric_fn, aggregate_fn=torch.mean):\n",
    "    model.eval()\n",
    "    metrics=[]\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric = metric_fn(y_pred, y_batch)\n",
    "            metrics.append(metric)\n",
    "    return aggregate_fn(torch.stack(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "119fcc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3346, device='cuda:0')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = TensorDataset(X_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_set, batch_size=32)\n",
    "valid_mse = evaluate(model, valid_loader, mse)\n",
    "valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bc4493c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5589, device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to use rmse\n",
    "def rmse(pred, true):\n",
    "    return ((pred-true)**2).mean().sqrt()\n",
    "\n",
    "evaluate(model, valid_loader, rmse)\n",
    "\n",
    "# root(mse) != rmse because torch computed the mean rmse across sets, not the root of total mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c806d422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5784, device='cuda:0')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, valid_loader, mse, aggregate_fn=lambda metrics: torch.sqrt(torch.mean(metrics)))\n",
    "# Use MSE as metric function, and aggregate by taking the root of the mean mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f1311",
   "metadata": {},
   "source": [
    "## Nonsequential Models w/ Custom Modules\n",
    "\n",
    "Wide and Deep neural net: all/part of inputs are connected directly to the output layer, letting the model learn shallow and deep patterns\n",
    "\n",
    "We need to use `nn.Module` to build our custom network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "27101c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 50), nn.ReLU(),\n",
    "            nn.Linear(50,40), nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(40+n_features, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        deep_output = self.deep_stack(X)\n",
    "        wide_and_deep = torch.concat([X, deep_output], dim=1)\n",
    "        return self.output_layer(wide_and_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9be38e",
   "metadata": {},
   "source": [
    "Modules have a .children() method that lets you iterate over the submodules. If your model has a changing number of submodules, you should store them in an nn.ModuleList, and if you have a changing number of params, you should keep it in a nn.ParameterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5d68e953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.4628\n",
      "Epoch 2/20, Loss: 0.6090\n",
      "Epoch 3/20, Loss: 0.5629\n",
      "Epoch 4/20, Loss: 0.5364\n",
      "Epoch 5/20, Loss: 0.5209\n",
      "Epoch 6/20, Loss: 0.5116\n",
      "Epoch 7/20, Loss: 0.5048\n",
      "Epoch 8/20, Loss: 0.5010\n",
      "Epoch 9/20, Loss: 0.4959\n",
      "Epoch 10/20, Loss: 0.4978\n",
      "Epoch 11/20, Loss: 0.4879\n",
      "Epoch 12/20, Loss: 0.4893\n",
      "Epoch 13/20, Loss: 0.4851\n",
      "Epoch 14/20, Loss: 0.4802\n",
      "Epoch 15/20, Loss: 0.4772\n",
      "Epoch 16/20, Loss: 0.4758\n",
      "Epoch 17/20, Loss: 0.4706\n",
      "Epoch 18/20, Loss: 0.4687\n",
      "Epoch 19/20, Loss: 0.4684\n",
      "Epoch 20/20, Loss: 0.4601\n"
     ]
    }
   ],
   "source": [
    "model = WideAndDeep(n).to(device)\n",
    "lr = .002\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "train(model, optimizer, mse, train_loader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f713172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to send a subset of the features thru the wide path, and a different (mayb overlapping) part through the deep path, you can do smthn likethis:\n",
    "class WideAndDeep2(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 50), nn.ReLU(),\n",
    "            nn.Linear(50,40), nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(40+n_features, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_wide = X[:, :5]\n",
    "        X_deep = X[:, 2:]\n",
    "        deep_output = self.deep_stack(X_deep)\n",
    "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
    "        return self.output_layer(wide_and_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9bc72",
   "metadata": {},
   "source": [
    "### Making Models with Multiple Inputs\n",
    "\n",
    "Its usually better to just let the model take two tensors as input rather than trying to data split within the model. (like images + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a87c9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep3(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.deep_stack = nn.Sequential(\n",
    "            nn.Linear(n_features, 50), nn.ReLU(),\n",
    "            nn.Linear(50,40), nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(40+n_features, 1)\n",
    "\n",
    "    def forward(self, X_wide, X_deep):\n",
    "        deep_output = self.deep_stack(X_deep)\n",
    "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
    "        return self.output_layer(wide_and_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2b6a519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train[:, :5], X_train[:, 2:], y_train)\n",
    "train_loader_wd = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# you need to make sure your train and eval functions handle 3 tensors\n",
    "def evaluate(model, data_loader, metric_fn, aggregate_fn=torch.mean):\n",
    "    model.eval()\n",
    "    metrics=[]\n",
    "    with torch.no_grad():\n",
    "        for X_batch_wide, X_batch_deep, y_batch in data_loader:\n",
    "            X_batch_wide = X_batch_wide.to(device)\n",
    "            X_batch_deep = X_batch_deep.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch_wide, X_batch_deep)\n",
    "            metric = metric_fn(y_pred, y_batch)\n",
    "            metrics.append(metric)\n",
    "    return aggregate_fn(torch.stack(metrics))\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        for X_batch_wide, X_batch_deep, y_batch in train_loader:\n",
    "            X_batch_wide = X_batch_wide.to(device)\n",
    "            X_batch_deep = X_batch_deep.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch_wide, X_batch_deep)\n",
    "            loss = criterion(y_batch, y_pred)\n",
    "            total_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec674af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IFF your inputs are in the same order everywhere, you can use something like this: \n",
    "\n",
    "# for *X_batch_inputs, y_batch in data_loader:\n",
    "#     X_batch_inputs = [X.to(device) for X in X_batch_inputs]\n",
    "#     y_batch = y_batch.to(device)\n",
    "#     y_pred = model(*X_batch_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
