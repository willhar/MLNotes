{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325d6aa2",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "This chapter:  train/eval/finetine/optimize w/ pytorch. Then, optuna library for fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eebf98",
   "metadata": {},
   "source": [
    "## Fundamentals\n",
    "\n",
    "Data type is a tensor. Its a multi-dim array w/shape and datatype. Can live on GPU, and does auto-differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe3840",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58dd111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32245ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors only take one data type. If you give it more than one, the most general type will be selected (complex > float > int > bool)\n",
    "display(X.dtype)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a94ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66., 56.],\n",
       "        [56., 49.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntax\n",
    "X[:,1]\n",
    "10 * (X+1)\n",
    "X.exp() #item-wise exponential\n",
    "X.mean(dim=0) # col-wise mean\n",
    "X @ X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f101ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "torch.tensor(X.numpy(), dtype=torch.float32)\n",
    "# you can convert btwn numpy and torch easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c85c294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., 99.,  7.],\n",
       "        [ 2., 99.,  6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can modify in place\n",
    "X[:,1] = 99\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3049dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 99.,  7.],\n",
       "        [ 0., 99.,  6.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0] = -5\n",
    "X.relu_() \n",
    "# Methods ending in _ are in place, normal methods are not in place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c720f7",
   "metadata": {},
   "source": [
    "### Hardware Acceleration\n",
    "\n",
    "PyTorch has accelerator support for intel, apple, nvidia, amd, etc etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac47a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a gpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"We have a gpu\")\n",
    "\n",
    "M = torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float32)\n",
    "M = M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9292fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.device\n",
    "# There are multiple ways to put tensors on gpus, like .cuda(), or setting device= param in torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5300cf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = M @ M.T\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17660ac3",
   "metadata": {},
   "source": [
    "If your neural net is deep, GPU speed and RAM matters most, if its shallow, getting training data onto GPU is the bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d0e893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.9 ms ± 677 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000,1000))\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee3d1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543 µs ± 10.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "M = torch.rand((1000,1000), device=\"cuda\")\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8db1fc",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "PyTorch does reverse-mode auto-diff (ch9) quickly with a method called autograd (auto gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3411b4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True) # requires_grad tells pytorch to keep track of computations for backpropagation\n",
    "f = x**2 # keeps a grad_fn= argument to tell pytorch how to backpropagate through this\n",
    "f.backward() # computes gradients\n",
    "x.grad\n",
    "# the derivative of x**2 at x=5 is in fact 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3217e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do gradient descent, you need to tell pytorch not to track this step\n",
    "# Otherwise it would include it in backprop\n",
    "\n",
    "lr = .1\n",
    "with torch.no_grad():\n",
    "    x -= lr*x.grad\n",
    "\n",
    "# This code is equivalent: (x detached shares memory with x)\n",
    "# x_detached = x.detach()\n",
    "# x_detached -= lr*x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e3bcfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before you repeat the forward > backward > gradient descent step, need to set gradients to 0\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1038ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8026e-45, requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The whole training loop:\n",
    "lr = .1\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "for iter in range(500):\n",
    "    f = x**2\n",
    "    f.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= lr*x.grad\n",
    "    x.grad.zero_()\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7385a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use in-place operations to save memory you have to be careful\n",
    "# Autograd doesnt let you do an in-place op to a leaf node\n",
    "\n",
    "t = torch.tensor(2.0, requires_grad=True)\n",
    "Z = t.exp() # intermediate step\n",
    "Z+=1 # in place operation (pytorch has no idea where to keep the computation graph for both steps)\n",
    "# Z.backward() #-> throws error\n",
    "\n",
    "# you need to do Z = Z+1, it creates a new step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a8342",
   "metadata": {},
   "source": [
    "PyTorch stores different operations differently\n",
    "- exp(), relu(), sqrt(), sigmoid(), tanh() save output in computation graph during the forward pass. You cannot modify their output in place.\n",
    "- abs(), cos(), log() save their inputs, so you cant change whatever you input to them before the backward pass\n",
    "- max(), min(), sgn(), std() save inputs and outputs, so do not change their inputs or outputs in place before .backward()\n",
    "- ceil(), floor(), mean(), sum() store nothing. Do what you want\n",
    "\n",
    "Generally, make your models without in-place ops, then if you need to speed up or save memory you can convert to in-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004dce5f",
   "metadata": {},
   "source": [
    "## Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de74844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = fetch_california_housing(as_frame=False)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(data.data, data.target, test_size=.2)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02090d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "means = X_train.mean(dim=0, keepdims=True)\n",
    "stds = X_train.std(dim=0, keepdims=True)\n",
    "# stdizing\n",
    "X_train = (X_train-means)/stds\n",
    "X_valid = (X_valid-means)/stds\n",
    "X_test = (X_test-means)/stds\n",
    "\n",
    "y_train = torch.FloatTensor(y_train).reshape(-1,1)\n",
    "y_valid = torch.FloatTensor(y_valid).reshape(-1,1)\n",
    "y_test = torch.FloatTensor(y_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55f63121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 16.06003189086914\n",
      "Epoch 1 loss: 4.835346221923828\n",
      "Epoch 2 loss: 2.212664842605591\n",
      "Epoch 3 loss: 1.3021349906921387\n",
      "Epoch 4 loss: 0.952170193195343\n",
      "Epoch 5 loss: 0.806835949420929\n",
      "Epoch 6 loss: 0.7393255233764648\n",
      "Epoch 7 loss: 0.7026336789131165\n",
      "Epoch 8 loss: 0.6789199709892273\n",
      "Epoch 9 loss: 0.6612536311149597\n",
      "Epoch 10 loss: 0.6468537449836731\n",
      "Epoch 11 loss: 0.6345417499542236\n",
      "Epoch 12 loss: 0.6237688660621643\n",
      "Epoch 13 loss: 0.6142401099205017\n",
      "Epoch 14 loss: 0.6057689785957336\n",
      "Epoch 15 loss: 0.5982191562652588\n",
      "Epoch 16 loss: 0.5914807319641113\n",
      "Epoch 17 loss: 0.5854612588882446\n",
      "Epoch 18 loss: 0.5800800919532776\n",
      "Epoch 19 loss: 0.5752664804458618\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n = X_train.shape[1] # n features\n",
    "w = torch.randn((n,1), requires_grad=True) # weights\n",
    "b = torch.tensor(0., requires_grad=True) # biases\n",
    "\n",
    "lr = .4\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = X_train @ w + b\n",
    "    loss = ((y_pred - y_train) ** 2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        b -= b.grad * lr\n",
    "        w -= w.grad * lr\n",
    "        b.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    print(f\"Epoch {epoch} loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "288d10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5125],\n",
      "        [4.2779],\n",
      "        [1.8360]])\n"
     ]
    }
   ],
   "source": [
    "# Making predictions\n",
    "with torch.no_grad():\n",
    "    print(X_test[:3] @ w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179ee4c",
   "metadata": {},
   "source": [
    "This works but PyTorch has a higher level API to do all this easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d3881",
   "metadata": {},
   "source": [
    "#### PyTorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c1f594b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = nn.Linear(in_features=n, out_features=1)\n",
    "model.weight #.weight and .bias are children of torch.nn.Parameter, which is a child of torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc0a7cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3117], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2919933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5772]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train[:1])\n",
    "# not trained yet so predictions r random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pick an optimizier and a loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "mse = nn.MSELoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
